---
title: 深度卷积神经网络
tags:
  - 深度学习

---

- [ 深度卷积神经网络](#head1)
	- [ 基于深度卷积神经网路的图像网络分类](#head2)
		- [ 引言：](#head3)
		- [ TCP和UDP：](#head4)
	- [ 卷积神经网络（CNN）详解](#head5)
		- [ 1、卷积神经网络结构介绍](#head6)
		- [ 2、构建卷积神经网络的各种层](#head7)
			- [ 2.1、卷积层](#head8)
				- [ 2.1.1、卷积层的作用](#head9)
				- [ 2.1.2、感受野（重点理解）](#head10)
				- [ 2.1.3、神经元的空间排列](#head11)
				- [ 2.1.4、权值共享](#head12)
				- [ 2.1.5、卷积层的超参数及选择](#head13)
				- [ 2.1.6、卷积层演示](#head14)
				- [ 2.1.7、用矩阵乘法计算卷积](#head15)
				- [2.1.8 其它形式的卷积操作](#head16)
			- [2.2 池化层](#head17)
			- [ 2.3、归一化层](#head18)
			- [ 2.4、全连接层](#head19)
				- [ 2.4.1、将卷积层转化为全连接层](#head20)
				- [ 2.4.2、将全连接层转化为卷积层](#head21)


# <span id="head1"> 深度卷积神经网络</span>
<img src="/assets/image/image-20210913165142737.png"/>

图像的卷积操作的关键：就是要把卷积当作是过去对现在的影响，周围的像素点对当前像素点的影响，g函数就是规定了如何影响的关键。就像是人的视网膜，看到图像之后，先经过一个预处理，然后再交给大脑。

卷积神经网络一般用来做图像识别，比如以下这种情况：

<img src="/assets/image/image-20210911192102591.png" style="zoom:33%;" />

通过卷积神经网络就能将他们识别出来是X和O。

重点是不是识别这种规规整整的情况，而是像那种不规整的X和O，这种情况是我们人能够一眼就看出来的，但是交给计算机就不行了。

<img src="/assets/image/image-20210911192331381.png" style="zoom:33%;" />

卷积神经网络识别图像的第一步就是把图像的局部特征给挑出来，然后把这些局部特征交给神经网络，由神经网络去判断。通过对图像就行卷积操作，来提取局部特征。

平滑卷积核的作用是处理一个像素点和周围像素点的关系。

水平卷积核（水平边界过滤器）会忽略垂直方向上的边界，只把左右方向的边界给挑出来；垂直卷积核（垂直边界过滤器）类似。这两种卷积核也被称为过滤器。

<img src="/assets/image/image-20210911193129684.png" style="zoom: 33%;" />

比如识别X的问题，使用以下三个卷积核就能把三个特征全部找出来

<img src="/assets/image/image-20210911194551909.png" style="zoom:33%;" />



<img src="/assets/image/image-20210911194128200.png"  style="zoom: 25%;" />

<img src="/assets/image/image-20210911200020587.png"  style="zoom:25%;" />

整个图像都计算后，就得到了与这个特征匹配的全部信息：

<img src="/assets/image/image-20210911200201875.png"  style="zoom: 33%;" />

然后再把三个特征的卷积核都分别再操作一遍：

<img src="/assets/image/image-20210911200354218.png" style="zoom:33%;" />

接下来神经网络就开始干活了， 神经网络通过对这些特征信息的判断，就可以进行图像识别了。



但是，人脸上有数万个特征，如果用数万个卷积核对原图进行处理，从而得到数万个feature map，那么运算量就会非常大，对于自动驾驶、人脸识别等要求毫秒级别的运算，是时候时间上就来不及了，因此就需要把图像进行缩小，**池化（也叫做下采样）**的所用就是把得到的feature map进行缩小，



<img src="/assets/image/image-20210912101445271.png"  style="zoom:25%;" />

<img src="/assets/image/image-20210912101249052.png"  style="zoom:25%;" />

池化的作用就是把得到的feature map中很大的一个框用一个数字来表示。  通过池化，大大减少了数据量，这也意味着在可接受范围内牺牲了一部分信息。

<img src="/assets/image/image-20210912103102358.png" style="zoom: 50%;" />

最大池化和平均池化运用都非常广泛，一般是用max pooling，因为它在保留了原图特征的同时，还能把图片的尺寸降下来：

<img src="/assets/image/image-20210912103347285.png"  style="zoom: 50%;" />

接下来，就是要把矩阵中的负数全部变成0。

<img src="/assets/image/image-20210912103748672.png"  style="zoom:50%;" />

这个叫做激活函数。

原图经过卷积、抹0、池化之后，就变成了下图的样子。中间的Convolution-ReLU-Pooling其实就是一个神经网络。

<img src="/assets/image/image-20210912104209303.png"  style="zoom: 50%;" />

这三个作为一个单元可以多次重复的排列组合：

<img src="/assets/image/image-20210912104512033.png"  style="zoom:50%;" />

卷积神经网络的可视化单元：https://www.cs.ryerson.ca/~aharley/vis/conv/

<img src="/assets/image/image-20210912105356940.png" style="zoom: 33%;" />

首先就是把图转换为像素点，然后对原图进行6个卷积，得到6个feature map，然后对这6个future map进行池化，得到6个小图，对小图再进行卷积，再进行池化，进行两层全连接，就得到了结果。



接下来是全连接：

<img src="/assets/image/image-20210912105516648.png"  style="zoom: 67%;" />

然后，每一个值都是有权重的，比如，当原图是X的时候，第一个数是1的概率就大一些，第一个数的权重就要大一些。

再将它们各自乘以权重（权重是通过训练获得的），加起来，就能判断出是X的概率是多少

<img src="/assets/image/image-20210912105903297.png" style="zoom:50%;" />

之所以叫全连接，是因为原图跟每一个神经元都相连的，每一个神经元乘以各自的权重，再加和，就得到了是原图的概率。

```
imagenet：图片数据集网站
```

​		通过用大量的图片去训练模型，通过一种叫做反向传播的算法（Backpropagation），就是说神经网络先得出一个结果，然后把这个结果跟真实的结果进行比较，进行一个误差的计算，这就叫做损失函数。我们的目标是把损失函数降到最低，通过修改卷积核的参数，修改全全连接每一个神经元的权重，来进行微调，一层一层把误差反馈回去，所以叫反向传播。也就是说，最后可能它的误差要反馈到第一个卷积核上的，来对卷积核的参数进行修改。

​		通过大量图片的训练，那么最后它自己就学会了采用哪些卷积核、全连接中的每个神经元的权重是多少，自动就学会了，就叫机器学习。并非是人为指定一个卷积核，识别出鼻子、嘴这样一个特征，而是通过自己训练，自己知道它要得到一个什么样的特征，而不需要认为为它举例，这也是神经网网络强大的一个地方---机器具有学习能力，人只需要给它大量的数据就好了。

------

<img src="/assets/image/image-20210912113336010.png"  style="zoom:50%;" />

目标就是把误差降到最小，

<img src="/assets/image/image-20210913085908732.png" style="zoom:33%;" />

梯度下降函数（算法）通过求导，找出最低值，将损失函数降到最低，就表示学习成功了。

初始参数（超参数，叫做Hyperparameters），事先指定好卷积核的尺寸和数目，用随机生成一些数字放到卷积核里面，像卷积核的数目、卷积核的大小、池化的步长、大小、全连接层神经元的数量-------这些是人为定好的，是机器无法学习的。通过喂大量的图片，来将参数调到最佳。

如声音也能转换为像图像数据结构一样的数据（句子等），由卷积神经网络来处理；视频能够提取其中一帧一帧的数据来处理。

卷积神经网络只能处理这种数据结构呈现出图片一样的数据

像下图这样列可以互换的，<span style='color:red;'>不能用卷积神经网络来处理！</span>

<img src="/assets/image/image-20210913164302468.png"  style="zoom:50%;" />

卷积神经网络在发现特征，分类图片上运用。

牛津大学的VGG模型是2014年ILSVRC竞赛的第二名，第一名是GoogLeNet，但是VGG模型在多个迁移学习任务中的表现要优于GoogleNet。而且，从图像中提取CNN特征，VGG模型是首选算法。它的缺点在于，参数量有140M之多，需要更大的存储空间，但是这个模型很有研究价值。

<img src="/assets/image/image-20210913164759871.png"  style="zoom:50%;" />

对于每个图片，在卷积神经网络经过处理后，会给出5个比较像的答案。top1就是图片本身答对了的概率，top5就表示它猜的前五个里面有正确答案的概率。可以看到上图的错误率是逐年下降的。

## <span id="head2"> 基于深度卷积神经网路的图像网络分类</span>

#### <span id="head3"> 引言：</span>

我们训练了一个大型深度卷积神经网络，将ImageNet  LSVRC-2010竞赛中的120万张高分辨率图像分类为1000个不同类别。在测试数据上，我们取得了top-1和top-5的错误率分别为37.5%和17.0%，大大优于之前的国家技术水平。该神经网络有6000万个参数和65万个神经元，由5个卷积层组成，其中一些卷积层后面是最大池层，以及3个完全连接层，最终具有1000路softmax。为了使训练更快，我们使用非饱和神经元和一个非常高效的GPU实现卷积操作。为了减少全连通层的过拟合，我们采用了最近发展的正则化方法“dropout”，该方法被证明是非常有效的。我们还在ILSVRC-2012竞赛中加入了该模型的一个变体，并获得了前5名的15.3%的测试错误率，而第二好的参赛作品的错误率为26.2%。

<span style='color:red;'>非饱和神经元还未理解！！</span>

------

#### <span id="head4"> TCP和UDP：</span>

<img src="/assets/image/image-20210905201655869.png"  style="zoom:50%;" />


**神经网络**：神经网络是一组按层次组成的神经元。每个神经元都是一个数学计算，它接受输入，乘以它的权重，然后通过激活函数将总和传递给其他神经元。神经网络正在学习如何根据前面的例子调整输入的权重来对输入进行分类。

**过拟合**：过拟合是指为了得到一致假设的而使假设变得过度严格。避免过拟合是分类器设计中的一个核心任务。通常采用增大数据量和测试样本集的方法对分类器性能进行评价。

**欠拟合**：欠拟合是指模型拟合程度不高，数据距离拟合曲线较远，或指模型没有很好的捕捉到数据特征，不能够很好的拟合数据。

**卷积层**：卷积神经网络中每层卷积层由若干卷积单元组成，每个卷积单元的参数都是通过反向传播算法最佳化得到的。卷积运算的目的是提取输入的不同特征，第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网络能从低级特征中迭代提取更复杂的特征。

**最大池化**：最大池化（max-pooling）即局部接收域中值最大的点。

> 常用的池化方法有最大池化（max-pooling）和均值池化（mean-pooling）。根据相关理论，特征提取的误差主要来自两个方面：
>
> （1）领域大小受限造成的估计值方差增大。
>
> （2）卷积层参数误差造成估计均值的偏移。
>
> 一般来说，mean-pooling能减小第一种误差，更多的保留图像的背景信息，max-pooling能减小第二种误差，更多的保留纹理信息。与mean-pooling近似，在局部意义上，则服从max-pooling的准则。
>
> max-pooling卷积核的大小一般是2X2。非常大的输入量可能需要4X4。但是，选择较大的形状会显著降低信号的尺寸，并可能导致信息过度丢失。通常，不重叠的池化窗口表现最好。

**非饱和神经元**：

------

**全连接层**：在整个卷积神经网络中起到“分类器”的作用。如果说卷积层、池化层和激活函数层等操作是将原始数据映射到隐层特征空间的话，全连接层则起到将学到的“分布式特征表示”映射到样本标记空间的作用。在实际使用中，全连接层可由卷积操作实现：对前层是全连接的连接层可以转化为卷积核为1X1的卷积；而前层是卷积层的全连接层可以转化为卷积核为hxw的全局卷积，h和w分别为前层卷积结果的高和宽。

https://www.zhihu.com/question/41037974

------

**dropout**：能够有效缓解过拟合的方法，在一定程度上达到正则化的效果。

https://blog.csdn.net/program_developer/article/details/80737724

最近引入的技术叫做“dropout”[10]，它包括将每个隐藏神经元的输出以0.5的概率设为零。以这种方式“退出”的神经元不参与前向传递，也不参与反向传播。因此，每当一个输入出现时，神经网络就会采样一个不同的架构，但所有这些架构都共享权重。这种技术减少了神经元复杂的共适应，因为一个神经元不能依赖于其他特定神经元的存在。因此，它被迫学习更健壮的特征，这些特征在与其他神经元的许多不同随机子集结合时是有用的。

**softmax**：	激活函数，被用于多分类过程中，它将多个神经元的输出，映射到（0，1）区间内，可以看成概率来理解，从而来进行多分类。

https://www.zhihu.com/question/23765351

**非饱和神经元**：

------

## <span id="head5"> 卷积神经网络（CNN）详解</span>

### <span id="head6"> 1、卷积神经网络结构介绍</span>

如果用全连接神经网络处理大尺寸图像具有3个明显的缺点：

- 首先将图像展开为向量会丢失空间信息
- 其次参数过多效率低下，训练困难
- 通过大量的参数也很快会导致网络过拟合

而卷积神经网络可以很好的解决上面三个问题

​		与常规网络不同，卷积神经网络的各层中的神经元是三维排列的：宽度、高度和深度。其中的宽度和高度都容易理解，因为本身卷积就是一个二维模板，但是在卷积神经网络中的深度指的是**激活数据体**的第三个维度，而不是整个网络的深度，整个网络的深度指的是网络的层数。层中的神经元将只与前一层中的一小块区域连接，而不是采用全连接方式。

<img src="/assets/image/image-20210908190758403.png"  style="zoom:67%;" />

如上图，对于用来分类的CIFAR-10中的图像的卷积网络，其最后的输出层的维度是1X1X10，因为在卷积神经网络结构的最后部分将会把全尺寸的图像压缩为包含分类评分的一个向量，向量是在深度方向排列的。

> 红色的输出层代表输入图像，所以它的宽度和高度就是图像的宽度和高度，它的深度是3（代表了红、绿、蓝三种颜色通道），与红色相邻的蓝色部分是经过卷积和池化之后的激活值（也可以看作是神经元），后面是接着的卷积池化层。

### <span id="head7"> 2、构建卷积神经网络的各种层</span>

​		卷积神经网络主要由这几层构成：输入层、卷积层、ReLU层、池化（pooling）层和全连接层（全连接层和常规神经网络中的一样）。通过将这些层叠加起来，就可以构建一个完整的卷积神经网络。在实际应用中往往将卷积层与ReLU层共同称之为卷积层，**所以卷积层经过卷积操作也是要经过激活函数的**。具体来说，卷积层和全连接层（CONV/FC）对输入执行变换操作的时候，不仅会用到激活函数，还会用到很多参数，即神经元的权值w和偏差b；而ReLU层和池化层则是进行一个固定不变的函数操作。卷积层和全连接层中的参数会随着梯度下降被训练，这样卷积神经网络计算出的分类评分就能和训练集中的每个图像的标签吻合了。

#### <span id="head8"> 2.1、卷积层</span>

卷积层是构建卷积神经网络的核心层，它产生了网络中大部分的计算量。

##### <span id="head9"> 2.1.1、卷积层的作用</span>

- **滤波器的作用或者说是卷积的作用。**卷积层的参数是有一些可学习的滤波器集合构成的。每个滤波器在空间上（宽度和高度）都比较小，**但是深度和输入数据一致**。直观的说，网络会让滤波器学习到当它看到某些类型的视觉特征时就激活，具体的视觉特征可能是某些方位上的边界，或者在第一层某些颜色的斑点，甚至可以是网络更高层上的蜂窝状或者车轮状图案。
- **可以被看作是神经元的一个输出**。神经元只观察输入数据中的一小部分，并且和空间上左右两边的所有神经元共享参数，因为这些数字都是使用同一个滤波器得到的结果。
- **降低参数的数量**。这个由于卷积具有“权值共享”这样的特性，可以降低参数数量，达到降低计算开销，防止由于参数过多而造成过拟合。

##### <span id="head10"> 2.1.2、感受野（重点理解）</span>

​		在处理图像这样的高纬度输入时，让每个神经元都与前一层中的所有神经元进行全连接是不现实的。相反，我们让每个神经元只与输入数据的一个局部区域连接。<span style='color:red;'>该连接的空间大小叫做神经元的感受野（receptive field），它的尺寸是一个超参数（其实就是滤波器的空间尺寸）。在深度方向上，这个连接的大小总是和输入量的深度相等</span>。需要再次强调的是，我们对待空间维度（宽和高）与深度维度是不同的：连接在空间（宽高）上是局部的，但是在深度上总是和输入数据的深度一致，这一点会在下面举例具体说明。

<img src="https://pic4.zhimg.com/v2-94792663768ebde313002cdbedb5297f_r.jpg" alt="preview" style="zoom: 33%;" />

> 在图中展现的是卷积神经网络的一部分，其中红色的为输入数据，假设输入数据体尺寸为[32X32X3]，如果感受野（或滤波器尺寸）是5X5，那么卷积层中的每个神经元会有输入数据体中[5X5X3]区域的权重，共有5X5X3=75个权重（还要加一个偏差参数）。深度列也被人称为纤维。
>
> 需要注意的是，这个连接在深度维度上的大小必须为3，和输入数据体的深度一致。其中还有一点需要注意，对应一个感受野有75个权重，这75个权重是通过学习进行更新的，所以很大程度上这些权重之间是不相等（也就对于一个卷积核，它对于与它连接的输入的每一层的权重都是独特的，不是同样的权重重复输入层层数那么多次就可以的）。在这里相当于前面的每一层对应一个传统意义上的卷积模板，每一层与自己卷积模板做完卷积之后，再将各个层的结果加起来，再加上偏置，注意是<span style='color:red;'>一个偏置</span>，无论输入数据是多少层，一个卷积核就对应一个偏置。

##### <span id="head11"> 2.1.3、神经元的空间排列</span>

感受野讲解了卷积层中每个神经元与输入数据体之间的连接方式，但是尚未讨论输出数据体中神经元的数量，以及它们的排列方式。3个超参数控制着输出数据体的尺寸：深度、步长和零填充。

- 输出数据体的深度：它是一个超参数，和使用的滤波器的数量一致，而每个滤波器在输入数据中寻找一些不同的东西，即图像的某些特征。
- 在滑动滤波器的时候，必须指定步长。当步长为1，滤波器每次移动1个像素；当步长为2，滤波器滑动时每次移动2个像素，当然步长也可以是不常用的3，或者更大的数字，但这些在实际中很少使用。这个操作会让输出数据体在空间上变小。
- 有时候将输入数据体用0在边缘处进行填充是很方便的。这个零填充的尺寸是一个超参数。零填充有一个良好性质，即可以控制输出数据体的空间尺寸（最常用的是用来保持输入数据体在空间上的尺寸，使得输入和输出的宽高都相等）。

<img src="/assets/image/image-20210911090135086.png"  style="zoom: 67%;" />

一般来说，当步长S=1时，零填充的值是P=（F-1）/2，这样就能保证输入和输出数据体有相同的空间尺寸。

**步长的限制**：注意这些空间排列的超参数之间是相互限制的。

> 举例来说，当输入尺寸W=10，不使用零填充P=0,滤波器尺寸F=3,此时步长S=2是行不通的，因为（W-F+2P）/S+1=(10-3+0)/2+1=4.5，结果不是整数，这就是说神经元不能整齐对称地滑过输入数据体.因此这些超参数的设定就被认为是无效的，一个卷积神经网络库可能会报出一个错误，通过修改零填充值、修改输入数据体尺寸，或者其他什么措施来让设置合理。在后面的卷积神经网络结构小结中，可以看到合理地设置网络地尺寸让所有的维度都能正常工作，是非常让人头疼的事；而使用零填充和遵循其他一些设计策略将会有效解决这个问题。

##### <span id="head12"> 2.1.4、权值共享</span>

在卷积层中，权值共享是用来控制参数的数量。假如在一个卷积核中，每一个感受野采用地都是不同的权重值（卷积核的值不同），那么这样的网络中参数数量将是十分巨大的。

>  权值共享是基于这样一个合理的假设：如果一个特征在计算某个空间位置（x1,y1）的时候有用，那么它在计算另一个不同位置（x2,y2）的时候也有用。基于这个假设，可以显著的减少参数数量。换言之，就是将深度维度上一个单独的2维切片看作是深度切片，比如一个数据体尺寸为[55X55X96]的就有96个深度切片，每个尺寸为[55X55]，其中在每个深度切片上的结果都使用相同的权重和偏差来获得的。在这样的参数共享下，假如一个例子中的第一个卷积层有96个卷积核，那么就有96个不同的权重集了，一个权重集对应一个深度切片，如果卷积核大小是11X11的，图像是RGB 3通道的，那么就共有96X11X11X3=34848个不同的权重，总共有34944个参数（因为要加96个偏差），并且在每个深度切片中的55X55的结果使用的都是相同的参数。

在反向传播的时候，都要计算每个神经元对它权重的梯度，但是需要把同一个深度切片上的所有神经元对权重的梯度累加，这样就得到了对共享权重的梯度。这样，每个切片只更新一个权重集。这样做的原因可以通过下面的这张图片解释：（将卷积层用全连接的方式表示）

<img src="https://pic1.zhimg.com/v2-9cb091229562146799d69b05cd2c02b8_r.jpg" alt="preview" style="zoom:33%;" />

> 如上图所示，左侧的神经元是将每一个感受野展开为一列之后串联起来（就是展开排成一列，同一层神经元之间不连接）。右侧的Deep1i是深度为1的神经元的第i个，Deep2i是深度为2的神经元的第i个，同一层深度神经元的权值都是相同的，黄色的都是相同的（上面4个与下面4个的参数相同），蓝色都是相同的。所以现在回过头来看上面说的卷积神经网络的反向传播公式对梯度进行累加求和也是基于这点考虑（同一深度的不同神经元共用一组参数，所以累加）；而每个切片只更新一个权重集的原因也是这样的，因为从上图可以看到，<span style='color:red;'>不同深度的神经元不会公用相同的权重</span>，所以只能更新一个权重集。

- **需要注意的是**，如果在一个深度切片中的所有权重都使用同一个权重变量，那么卷积层的前向传播在每个深度切片中可以看作是在计算神经元权重和输入数据体的卷积（这就是卷积层名字的由来）。这也是为什么总是将这些权重集合称为滤波器（或卷积核），因为它们和输入进行了卷积。
- **注意**，有时候参数共享假设可能没有意义，特别是卷积神经网络的输入图像是一些明确的中心结构的时候。这时候我们就应该期望在图片的不同位置学习到完全不同的特征（而一个卷积核的滑动与图像做卷积都是在学习相同的特征）。一个具体的例子就是输入图像是人脸，人脸一般在图像的中心，而我们期待在不同的位置学习到不同的特征，比如眼睛特征或者头发特征可能会在图片的不同位置被学习。在这个例子中，通常就放松参数共享的限制，将层称为局部连接层。

##### <span id="head13"> 2.1.5、卷积层的超参数及选择</span>

<img src="/assets/image/image-20210914082524023.png"  style="zoom: 67%;" />

##### <span id="head14"> 2.1.6、卷积层演示</span>

因为3D数据难以可视化，所以所有的数据（输入数据体是蓝色，权重数据体是红色，输出数据体是绿色）都采用将深度切片按照列的方式排列展现。输入数据体的尺寸是W1=5,H1=5,D1=3，卷积层参数K=2，F=3，S=2，P=1。就是说，有两个滤波器，滤波器的尺寸是3X33X3，它们的步长是2。因此，输出数据体的空间尺寸是（5-3+2）/2+1=3。注意输入数据体使用了零填充P=1，所以输入数据体外边缘一圈都是0。下面的例子在绿色的输出激活数据上循环演示，展示了其中每个数据都是先通过蓝色的输入数据和红色的滤波器逐元素相乘，然后再求其总和，最后加上偏差得来。

<img src="/assets/image/image-20210914085910293.png"  style="zoom:50%;" />

##### <span id="head15"> 2.1.7、用矩阵乘法计算卷积</span>

卷积运算本质上就是在滤波器和输入数据的局部区域做点积。卷积层的常见实现方式就是利用这一点，将卷积层得前向传播变成一个巨大的矩阵算法。

（1）输入图像的局部区域被im2co操作拉伸为列。比如输入是[227X227X3]，要与尺寸为11X11X3的滤波器以步长为4进行卷积，就依次取输入中的[11X11X3]数据块，然后将其拉伸为长度为11X11X3=363的列向量。重复进行这一过程，因为步长为4，所以经过卷积后的宽和高均为(227-11)/4+1=55，共有55X55=3025个神经元。因为每一个神经元实际上都是对应有363的列向量构成的感受野，即一共要从输入上取出3025个363维的列向量。所以经过im2col操作得到的输出矩阵Xcol的尺寸是[363X3025]，其中每列是拉伸的感受野。注意因为感受野之间有重叠，所有输入数据体中的数字在不同的列中可能有重复。

（2）卷积层的权重也同样被拉伸成行。例如，如果有96个尺寸为[11X11X3]的滤波器，就生成一个矩阵Wrow，尺寸为[96X363]。

（3）现在卷积的结果和进行一个大矩阵乘法np.dot(Wrow,Xcol)是等价的了，能得到每个滤波器和每个感受野间的点积输出。在例子中，这个操作的输出是[96X3025]，给出了每个滤波器在每个位置上的点积输出。注意其中的np.dotnp.dot计算的是矩阵乘法而不是点积。

（4）结果最后必须被重新变为合理的输出尺寸[55X55X96]。

这个方法的缺点是占用内存过多，因为在输入数据体中的某些值在Xcol中被重复了多次；优点在于矩阵乘法有非常多的高效底层实现方式（比如常见的BLAS API）。还有，同样的im2col思路可以用在池化操作中。反向传播：卷积操作的反向传播（同样对于数据和权重）还是一个卷积（但是和空间上翻转的滤波器）。使用一个1维的例子比较容易演示。

##### <span id="head16">2.1.8 其它形式的卷积操作</span>

**1X1卷积**：在卷积神经网络中，因为对三个维度进行操作，滤波器和输入数据体的深度是一样的。比如输入是[32X32X3]，那么1X1卷积就是在高效的进行3维点积（因为输入深度是3个通道）；另外一种想法是将这种卷积的结果看作是全连接层的一种实现方式。

**扩张卷积**：最近一个研究（Fisher Yu和Vladlen Koltun的论文）给卷积层引入了一个新的叫扩张（dilation）的超参数。到目前为止，我们只讨论了卷积层滤波器是连续的情况。但是，让滤波器中元素之间有间隙也是可以的，这就叫做扩张。如图为进行1扩张。

<img src="/assets/image/image-20210914095732758.png"  style="zoom:50%;" />

在某些设置中，扩张卷积与正常卷积结合起来非常好用，因为在很少的层数内更快的汇集输入图片的大尺度特征。比如，如果上下重叠2个3X3的卷积层，那么第二个卷积层的神经元的感受野是输入数据体中5X5的区域（可以看成这些神经元的有效感受野是5X5，如上图所示）。如果我们对卷积进行扩张，那么这个有效感受野就会迅速增长。

#### <span id="head17">2.2 池化层</span>

通常在连续的卷积层之间会周期性地插入一个池化层。它的作用是逐渐降低数据体的空间尺寸，这样的话就能减少网路中参数的数量，使得计算资源耗费变少，也能有效控制过拟合。汇聚层使用MAX操作，对输入数据体的每一个深度切片独立进行操作，改变它的空间尺寸。最常见的形式是汇聚层使用尺寸2X2的滤波器，以步长为2来对每个深度切片进行降采样，将其中75%的激活信息都丢掉。每个MAX操作是从4个数字中取最大值（也就是在深度切片中某个2X2的区域），深度保持不变。

汇聚层的一些公式：输入数据体尺寸 W1 X H1 X D1,有两个超参数：空间大小F和步长S；输入数据体的尺寸 W2 X H2 X D2，其中

<img src="/assets/image/image-20210914155420303.png" style="zoom:67%;" />

这里面与之前的卷积的尺寸计算的区别主要在于两点，首先在池化的过程中基本不会进行另补充；其次池化前后深度不变。



在实践中，最大池化层通常只有两种形式：一种是F=3,S=2,也叫重叠汇聚(overlapping pooling)，另一个更常用的是F=2,S=2。对更大感受野进行池化需要的池化尺寸也更大，而且往往对网络具有破坏性。

**普通池化（General Pooling）：**除了最大池化，池化单元还可以使用其他的函数，比如平均池化（average pooling）或L-2范式池化（L2-norm pooling）。平均池化历史上比较常用，但是现在已经很少使用了。因为实践证明，最大池化的效果比平均池化要好。

**反向传播：**回顾一下反向传播的内容，其中max（x,y）函数的反向传播可以简单理解为将梯度只沿最大的数回传。因此，在向前传播经过汇聚层的时候，通常会把池中最大元素的索引记录下来（有时这个也叫作道岔（switches）），这样在反向传播的时候梯度的路由就很高效。

**不使用汇聚层：**有发现认为，在训练一个良好的生成模型时，弃用汇聚层也是很重要的，通过在卷积层中使用更大的步长来降低数据体的尺寸。现在看起来，未来的卷积神经网络结构中，可能会很少使用甚至不使用汇聚层。

#### <span id="head18"> 2.3、归一化层</span>

在卷积神经网络的结构中，提出了不同类型的归一化层，有时候是为了实现在生物大脑中观测到的抑制机制。但是这些层渐渐都不再流行，因为实践证明他们的效果即使存在，也是极其有限的。

#### <span id="head19"> 2.4、全连接层</span>

这个常规神经网络中一样，它们的激活可以先用矩阵乘法，再加上偏差。

##### <span id="head20"> 2.4.1、将卷积层转化为全连接层</span>

对于任一个卷积层，都存在一个能实现和它一样的前向传播函数的全连接层。该全连接层的权重是一个巨大的矩阵，除了某些特定块（感受野），其余部分都是零；而在非 0 部分中，大部分元素都是相等的（权值共享）。

##### <span id="head21"> 2.4.2、将全连接层转化为卷积层</span>

任何全连接层都可以被转化为卷积层。比如一个K=4096的全连接层，输入数据体的尺寸是7X7X512，这个全连接层可以被等效的看作一个F=7,P=0,K=4096的卷积层。换句话说，就是将滤波器尺寸设置为和输入数据体的尺寸设为一致的。因为只有一个单独的深度列覆盖并滑过输入数据体，所以输出将变成1X1X4096，这个结果就和使用初始的那个全连接层一样了。这个实际上也很好理解，因为，对于其中的一个卷积滤波器，这个滤波器的深度为512，也就是说，虽然这个滤波器的输出只有1个，但是它的权重有7X7X512，相当于卷积滤波器的输出为一个神经元，这个神经元与上一层的所有神经元相连接，而这样与前一层所有神经元相连接的神经元一共有4096个，这就是一个全连接网络。

再上述的两种变换中，将全连接层转化为卷积层在实际运用中更加有用。假设一个卷积神经网络的输入是224X224X3的图像，一系列的卷积层和汇聚层将图像数据变为尺寸为7X7X512的激活数据体。在AlexNet中就是这样，通过使用5个汇聚层来对输入数据进行空间上的降采样，每次尺寸下降一半，所以最终空间尺寸为224/2/2/2/2/2=7。从这里可以看到，AlexNet使用了两个尺寸为4096的全连接层，最后一个有1000个神经元的全连接层用于计算分类评分。我们可以将这3个全连接转化为3个卷积层：

(1) 针对第一个连接区域是[7x7x512]的全连接层，令其滤波器尺寸为F=7，这样输出数据体就为[1x1x4096]了。

(2) 针对第二个全连接层，令其滤波器尺寸为F=1，这样输出数据体为[1x1x4096]。

(3) 对最后一个全连接层也做类似的，令其F=1，最终输出为[1x1x1000]。

这样做的目的是让卷积网络在一张更大的输入图片上滑动，得到多个输出，这样的转化可以让我们在单个向前传播的过程中完成上述的操作。

举个例子，如果我们想让224x224尺寸的浮窗，以步长为32在384x384的图片上滑动，把每个经停的位置都带入卷积网络，最后得到6x6个位置的类别得分。上述的把全连接层转换成卷积层的做法会更简便。如果224x224的输入图片经过卷积层和汇聚层之后得到了[7x7x512]的数组，那么，384x384的大图片直接经过同样的卷积层和汇聚层之后会得到[12x12x512]的数组（因为途径5个汇聚层，尺寸变为384/2/2/2/2/2 = 12）。然后再经过上面由3个全连接层转化得到的3个卷积层，最终得到[6x6x1000]的输出（因为(12 - 7)/1 + 1 = 6）。这个结果正是浮窗在原图经停的6x6个位置的得分！

面对384x384的图像，让（含全连接层）的初始卷积神经网络以32像素的步长独立对图像中的224x224块进行多次评价，其效果和使用把全连接层变换为卷积层后的卷积神经网络进行一次前向传播是一样的。自然，相较于使用被转化前的原始卷积神经网络对所有36个位置进行迭代计算，使用转化后的卷积神经网络进行一次前向传播计算要高效得多，因为36次计算都在共享计算资源。

这里有几个问题，首先是为什么以32为步长，如果以64为步长呢？再或者如果想用步长小于32的浮窗怎么办？

首先回答其中的一个问题。这是因为其中一个有五个汇聚层，因为25=32，也就是在原始图像上的宽或高增加32个像素，经过这些卷积核汇聚后，将变为一个像素。现在进行举例说明，虽然例子并没有32那么大的尺寸，但是意义都是一样的。假设原始图像的大小为4X4，卷积核F=3,S=1,P=1,而较大的图像的尺寸为8X8，假设对图像进行两层的卷积池化，在较大的图像上以步长为4进行滑动（22=4），如下图所示：
<img src="/assets/image/image-20210916113738159.png" style="zoom: 33%;" />
